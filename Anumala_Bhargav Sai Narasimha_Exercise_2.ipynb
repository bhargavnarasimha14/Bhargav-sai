{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# write your answer here\n",
        "Reasearch Question- Analyzing book pricing across different generes in a website\n",
        "Data Required- To analyze the book pricing fo different generes we need to collect data like title of the book, price of the book and genere of the\n",
        "book\n",
        "Amount of data required- For simple analysis we need a data set of 1000 books with their title, price and genere. This data can help us to identify\n",
        "trends over pricing and genere.\n",
        "Steps to collect and save the data-\n",
        "1. Identifying website to scrape data- We have to choose a website from which we can perform scraping and it should have data of price and genere of the book\n",
        "2. Setting up environent- We have to install python libraries such as BeautifulSoup and pandas\n",
        "3. Web Scraping- Understanding the HTML of web page and performing scraping using soup for parsing the webpage and requests to connect with the webpage\n",
        "4. Saving data- Convert the collected data using pandas dataframe and save it as a csv file and you can download the csv using google colab's\n",
        "   files.download()"
      ],
      "metadata": {
        "id": "uQB8CHwk4lJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "# List for storing book data\n",
        "book_data_list= []\n",
        "# URL for book store and page no is the url is dynamic\n",
        "web_url = \"http://books.toscrape.com/catalogue/page-{}.html\"\n",
        "for page_number in range(1, 51):  #website has 50 pages of books\n",
        "    url = web_url.format(page_number)\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        print(f\"Connected to the page no {page_number}!\")\n",
        "    else:\n",
        "        print(f\"Connection failed to the page {page_number}. Status_Code:{response.status_code}\")\n",
        "        break\n",
        "    # Using soap for parsing the web page\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    books = soup.find_all('article', class_='product_pod')\n",
        "    # Looping through the book details\n",
        "    for book in books:\n",
        "        title = book.h3.a['title']\n",
        "        price = book.find('p', class_='price_color').text\n",
        "        availability = book.find('p', class_='instock availability').text.strip()\n",
        "\n",
        "        # Using dictonary to store the book details\n",
        "        book_data = {\n",
        "            'title': title,\n",
        "            'price': price,\n",
        "            'availability': availability\n",
        "        }\n",
        "        book_data_list.append(book_data)\n",
        "    # Stopping at 1000 records\n",
        "    if len(book_data) >= 1000:\n",
        "        break\n",
        "data = book_data_list[:1000]\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('book_data.csv', index=False)\n",
        "files.download('book_data.csv')\n",
        "print(\"Web scraping completed\")"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        },
        "outputId": "344087ff-49c0-4e93-c3e5-edc97cdff54c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully connected to the page no 1!\n",
            "Successfully connected to the page no 2!\n",
            "Successfully connected to the page no 3!\n",
            "Successfully connected to the page no 4!\n",
            "Successfully connected to the page no 5!\n",
            "Successfully connected to the page no 6!\n",
            "Successfully connected to the page no 7!\n",
            "Successfully connected to the page no 8!\n",
            "Successfully connected to the page no 9!\n",
            "Successfully connected to the page no 10!\n",
            "Successfully connected to the page no 11!\n",
            "Successfully connected to the page no 12!\n",
            "Successfully connected to the page no 13!\n",
            "Successfully connected to the page no 14!\n",
            "Successfully connected to the page no 15!\n",
            "Successfully connected to the page no 16!\n",
            "Successfully connected to the page no 17!\n",
            "Successfully connected to the page no 18!\n",
            "Successfully connected to the page no 19!\n",
            "Successfully connected to the page no 20!\n",
            "Successfully connected to the page no 21!\n",
            "Successfully connected to the page no 22!\n",
            "Successfully connected to the page no 23!\n",
            "Successfully connected to the page no 24!\n",
            "Successfully connected to the page no 25!\n",
            "Successfully connected to the page no 26!\n",
            "Successfully connected to the page no 27!\n",
            "Successfully connected to the page no 28!\n",
            "Successfully connected to the page no 29!\n",
            "Successfully connected to the page no 30!\n",
            "Successfully connected to the page no 31!\n",
            "Successfully connected to the page no 32!\n",
            "Successfully connected to the page no 33!\n",
            "Successfully connected to the page no 34!\n",
            "Successfully connected to the page no 35!\n",
            "Successfully connected to the page no 36!\n",
            "Successfully connected to the page no 37!\n",
            "Successfully connected to the page no 38!\n",
            "Successfully connected to the page no 39!\n",
            "Successfully connected to the page no 40!\n",
            "Successfully connected to the page no 41!\n",
            "Successfully connected to the page no 42!\n",
            "Successfully connected to the page no 43!\n",
            "Successfully connected to the page no 44!\n",
            "Successfully connected to the page no 45!\n",
            "Successfully connected to the page no 46!\n",
            "Successfully connected to the page no 47!\n",
            "Successfully connected to the page no 48!\n",
            "Successfully connected to the page no 49!\n",
            "Successfully connected to the page no 50!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_75aca355-974b-4ce8-941a-abe319301094\", \"book_data.csv\", 59460)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Web scraping completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "847c2187-cf67-40cf-8b8e-036ff8a55c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping data for topic: cloud computing\n",
            "Scraping data for topic: devops\n",
            "Scraping data for topic: artificial intelligence\n",
            "Scraping data for topic: machine learning\n",
            "Scraping data for topic: natural language processing\n",
            "Scraping data for topic: open ai\n",
            "Scraping data for topic: web development\n",
            "Scraping data for topic: data science\n",
            "Scraping data for topic: data analytics\n",
            "Scraping data for topic: big data\n",
            "Scraping data for topic: frontend development\n",
            "Scraping data for topic: backend development\n",
            "Scraping data for topic: full stack development\n",
            "Scraping data for topic: mobile app development\n",
            "Scraping data for topic: iot\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_00481a61-7806-40ca-bb90-b12ad7d0b4e4\", \"research_topics_data.xlsx\", 177278)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data scraping completed\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "# Username and password for serp api of oxy labs\n",
        "USERNAME = \"Deepa_EaHaq\"\n",
        "PASSWORD = \"Deepa20020202_\"\n",
        "\n",
        "def get_html_for_page(url):\n",
        "    payload = {\n",
        "        \"url\": url,\n",
        "        \"source\": \"google\",\n",
        "    }\n",
        "    response = requests.post(\n",
        "        \"https://realtime.oxylabs.io/v1/queries\",\n",
        "        auth=(USERNAME, PASSWORD),\n",
        "        json=payload,\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    return response.json()[\"results\"][0][\"content\"]\n",
        "\n",
        "def parse_data_from_article(article):\n",
        "    title_elem = article.find(\"h3\", {\"class\": \"gs_rt\"})\n",
        "    title = title_elem.get_text()\n",
        "\n",
        "    title_anchor_elem = article.select(\"a\")[0]\n",
        "    url = title_anchor_elem[\"href\"]\n",
        "    # Extracting authors and venues\n",
        "    authors_and_venue = article.find(\"div\", {\"class\": \"gs_a\"}).get_text()\n",
        "    authors = \"Unknown\"\n",
        "    venue_info = \"Unknown\"\n",
        "    year = \"Unknown\"\n",
        "    try:\n",
        "        parts = authors_and_venue.split(\" - \")\n",
        "        authors = parts[0]\n",
        "        venue_info = parts[1]\n",
        "\n",
        "        # Extracting year\n",
        "        year = venue_info.split(\",\")[-1].strip()\n",
        "    except IndexError:\n",
        "        pass\n",
        "\n",
        "    # Extracting abstract data\n",
        "    abstract_elem = article.find(\"div\", {\"class\": \"gs_rs\"})\n",
        "    abstract = abstract_elem.get_text(strip=True) if abstract_elem else \"No abstract available\"\n",
        "\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"authors\": authors,\n",
        "        \"venue\": venue_info,\n",
        "        \"year\": year,\n",
        "        \"abstract\": abstract,\n",
        "        \"url\": url,\n",
        "    }\n",
        "\n",
        "def get_url_for_page(url, page_index):\n",
        "    return url + f\"&start={page_index}\"\n",
        "\n",
        "def get_data_from_page(url):\n",
        "    html = get_html_for_page(url)\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    articles = soup.find_all(\"div\", {\"class\": \"gs_ri\"})\n",
        "    return [parse_data_from_article(article) for article in articles]\n",
        "\n",
        "def scrape_data_for_topic(topic, num_pages=10):\n",
        "    base_url = f\"https://scholar.google.com/scholar?q={topic}&hl=en&as_sdt=0,5\"\n",
        "    data = []\n",
        "    page_index = 0\n",
        "\n",
        "    while len(data) < 1000:\n",
        "        page_url = get_url_for_page(base_url, page_index)\n",
        "        entries = get_data_from_page(page_url)\n",
        "        if not entries:  # breaking out of loop when no records are found\n",
        "            break\n",
        "        data.extend(entries)\n",
        "        page_index += 10\n",
        "        if page_index >= num_pages * 10:\n",
        "            break\n",
        "\n",
        "    return data\n",
        "\n",
        "# List of topics to scrape data from google scholar\n",
        "topics = [\n",
        "    \"cloud computing\",\n",
        "    \"devops\",\n",
        "    \"artificial intelligence\",\n",
        "    \"machine learning\",\n",
        "    \"natural language processing\",\n",
        "    \"open ai\",\n",
        "    \"web development\",\n",
        "    \"data science\",\n",
        "    \"data analytics\",\n",
        "    \"big data\",\n",
        "    \"frontend development\",\n",
        "    \"backend development\",\n",
        "    \"full stack development\",\n",
        "    \"mobile app development\",\n",
        "    \"iot\",\n",
        "    \"cybersecurity\",\n",
        "    \"blockchain\",\n",
        "    \"python\",\n",
        "    \"java\",\n",
        "    \"javascript\",\n",
        "]\n",
        "\n",
        "# Intializing data list\n",
        "all_data = []\n",
        "for topic in topics:\n",
        "    print(f\"Scraping data for topic: {topic}\")\n",
        "    topic_data = scrape_data_for_topic(topic)\n",
        "    all_data.extend(topic_data)\n",
        "    if len(all_data) >= 1000:\n",
        "        break\n",
        "\n",
        "# Limitting to 1000 records\n",
        "all_data = all_data[:1000]\n",
        "df = pd.DataFrame(all_data)\n",
        "excel_filename = 'research_topics_data.xlsx'\n",
        "df.to_excel(excel_filename, index=False)\n",
        "files.download(excel_filename)\n",
        "print(\"Data scraping completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "a6059c5f-519f-4a78-957e-ff7cc550a514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing instances: 100%|██████████| 16/16 [00:15<00:00,  1.01it/s]\n",
            "INFO:root:No instance specified, using random instance https://nitter.privacydev.net\n",
            "INFO:root:Current stats for MrBeast: 17 tweets, 0 threads...\n",
            "INFO:root:Current stats for MrBeast: 33 tweets, 0 threads...\n",
            "INFO:root:Current stats for MrBeast: 48 tweets, 0 threads...\n",
            "INFO:root:Current stats for MrBeast: 64 tweets, 0 threads...\n",
            "INFO:root:Current stats for MrBeast: 81 tweets, 0 threads...\n",
            "WARNING:root:Empty page on https://nitter.privacydev.net\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 tweets because of missing data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_14de0f27-3938-4d5f-8114-35fbdcffe20a\", \"MrBeast_tweets_data_randomized.xlsx\", 9378)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping completed\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from ntscraper import Nitter\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "# Intializing scrapper\n",
        "scraper = Nitter(log_level=1, skip_instance_check=False)\n",
        "tweets = scraper.get_tweets(\"MrBeast\", mode=\"user\", number=1000)\n",
        "# Intializing tweet data\n",
        "tweet_data = {\n",
        "    'URL': [],\n",
        "    'Tweet Text': [],\n",
        "    'Username': [],\n",
        "    'Likes Count': [],\n",
        "    'Quotes Count': [],\n",
        "    'Retweets Count': [],\n",
        "    'Comments Count': []\n",
        "}\n",
        "# Selecting random tweets\n",
        "tweet_limit = random.randint(30, 50)\n",
        "shuffled_tweets = random.sample(tweets['tweets'], len(tweets['tweets']))\n",
        "empty_fields_count = 0\n",
        "# Iterating throught shuffled tweets\n",
        "for i, tweet in enumerate(shuffled_tweets):\n",
        "    if i >= tweet_limit:\n",
        "        break\n",
        "    if not tweet['link'] or not tweet['text'] or not tweet['user']['name']:\n",
        "        empty_fields_count += 1\n",
        "        continue\n",
        "    tweet_data['URL'].append(tweet['link'])\n",
        "    tweet_data['Tweet Text'].append(tweet['text'])\n",
        "    tweet_data['Username'].append(tweet['user']['name'])\n",
        "    tweet_data['Likes Count'].append(tweet['stats']['likes'] if 'likes' in tweet['stats'] else 0)\n",
        "    tweet_data['Quotes Count'].append(tweet['stats']['quotes'] if 'quotes' in tweet['stats'] else 0)\n",
        "    tweet_data['Retweets Count'].append(tweet['stats']['retweets'] if 'retweets' in tweet['stats'] else 0)\n",
        "    tweet_data['Comments Count'].append(tweet['stats']['comments'] if 'comments' in tweet['stats'] else 0)\n",
        "# Logging\n",
        "print(f\"Skipped {empty_fields_count} tweets because of missing data.\")\n",
        "# Creating data frame to save to excel\n",
        "tweet_df = pd.DataFrame(tweet_data)\n",
        "output_filename = \"MrBeast_tweets_data_randomized.xlsx\"\n",
        "tweet_df.to_excel(output_filename, index=False)\n",
        "files.download(output_filename)\n",
        "print(\"Scraping completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The task is challenging, some of the key concepts i have explored is BeautifulSoup, request library which i have used frequently. I found\n",
        "BeautifulSoup quite benficial to extract data and the challenges i encountered while working on this assignment is rate limiting and blocking.\n",
        "I have used oxylabs serpscraper api to overcome these challenges and this assignment is highly relavent to my field of study for data collection and\n",
        "data science and also for some reasearch purposes.\n",
        "'''"
      ],
      "metadata": {
        "id": "B2QedvV8tXKZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}